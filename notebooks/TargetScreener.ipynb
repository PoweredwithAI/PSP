{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6bf3b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0  Obesity promotes ARDS by modulating ceramide t...   \n",
      "1  Adipose Tissue as a Target for Precision Medic...   \n",
      "2  A blueprint of synergistic effect in Crataegus...   \n",
      "3  Nanomaterials for the treatment and monitoring...   \n",
      "4  Reflection of therapy progress in virtual real...   \n",
      "\n",
      "                                            abstract pubYear  \n",
      "0  Obesity is an independent risk factor for acut...    2025  \n",
      "1  Following the trends of the adult obesity epid...    2025  \n",
      "2  <h4>Background</h4>Current obesity treatments ...    2025  \n",
      "3  Obesity represents a growing global health cri...    2025  \n",
      "4  <h4>Purpose</h4>Obesity is a major health chal...    2025  \n",
      "200\n",
      "{\"data\":{\"disease\":{\"id\":\"EFO_0001073\",\"name\":\"obesity\",\"associatedTargets\":{\"count\":2976,\"rows\":[{\"target\":{\"id\":\"ENSG00000166603\",\"approvedSymbol\":\"MC4R\"},\"score\":0.831734824240775,\"datasourceScores\":[{\"id\":\"eva\",\"score\":0.9405285610759455},{\"id\":\"gene_burden\",\"score\":0.9315446952667976},{\"id\":\"chembl\",\"score\":0.9159476985315466},{\"id\":\"gwas_credible_sets\",\"score\":0.7637295178016243},{\"id\":\"uniprot_literature\",\"score\":0.607930797611621},{\"id\":\"orphanet\",\"score\":0.607930797611621},{\"id\":\"impc\",\"score\":0.7522307566244891},{\"id\":\"europepmc\",\"score\":0.30281769235702505}]},{\"target\":{\"id\":\"ENSG00000175426\",\"approvedSymbol\":\"PCSK1\"},\"score\":0.7821655655695696,\"datasourceScores\":[{\"id\":\"eva\",\"score\":0.896268691753036},{\"id\":\"genomics_england\",\"score\":0.8860591375189377},{\"id\":\"uniprot_variants\",\"score\":0.8274613634158176},{\"id\":\"uniprot_literature\",\"score\":0.607930797611621},{\"id\":\"orphanet\",\"score\":0.607930797611621},{\"id\":\"gwas_credible_sets\",\"score\":0.4334102973459402},{\"id\":\"impc\",\"score\":0.5368990773882326},{\"id\":\"europepmc\",\"score\":0.10746527805113427}]},{\"target\":{\"id\":\"ENSG00000116678\",\"approvedSymbol\":\"LEPR\"},\"score\":0.7581176560062983,\"datasourceScores\":[{\"id\":\"eva\",\"score\":0.8609828728511453},{\"id\":\"genomics_england\",\"score\":0.8274613634158176},{\"id\":\"uniprot_variants\",\"score\":0.7599134970145264},{\"id\":\"gwas_credible_sets\",\"score\":0.6514507192634306},{\"id\":\"orphanet\",\"score\":0.607930797611621},{\"id\":\"uniprot_literature\",\"score\":0.607930797611621},{\"id\":\"chembl\",\"score\":0.4894794200745814},{\"id\":\"impc\",\"score\":0.7457377996464284},{\"id\":\"europepmc\",\"score\":0.21191875937690727}]},{\"target\":{\"id\":\"ENSG00000115138\",\"approvedSymbol\":\"POMC\"},\"score\":0.7528015445191536,\"datasourceScores\":[{\"id\":\"eva\",\"score\":0.8888677191445158},{\"id\":\"genomics_england\",\"score\":0.8274613634158176},{\"id\":\"uniprot_literature\",\"score\":0.6839221473130737},{\"id\":\"reactome\",\"score\":0.607930797611621},{\"id\":\"orphanet\",\"score\":0.607930797611621},{\"id\":\"impc\",\"score\":0.6743847509633882},{\"id\":\"europepmc\",\"score\":0.12678254433931982}]},{\"target\":{\"id\":\"ENSG00000112164\",\"approvedSymbol\":\"GLP1R\"},\"score\":0.7247871499766595,\"datasourceScores\":[{\"id\":\"chembl\",\"score\":0.9772641771597461},{\"id\":\"gwas_credible_sets\",\"score\":0.8078679004288501},{\"id\":\"impc\",\"score\":0.48834210156076097},{\"id\":\"europepmc\",\"score\":0.1709301925507239}]},{\"target\":{\"id\":\"ENSG00000174697\",\"approvedSymbol\":\"LEP\"},\"score\":0.7213620736862829,\"datasourceScores\":[{\"id\":\"eva\",\"score\":0.8449765856405652},{\"id\":\"genomics_england\",\"score\":0.7599134970145264},{\"id\":\"uniprot_variants\",\"score\":0.7599134970145264},{\"id\":\"orphanet\",\"score\":0.607930797611621},{\"id\":\"uniprot_literature\",\"score\":0.607930797611621},{\"id\":\"impc\",\"score\":0.6813594058498529},{\"id\":\"europepmc\",\"score\":0.2689625363187617}]},{\"target\":{\"id\":\"ENSG00000132170\",\"approvedSymbol\":\"PPARG\"},\"score\":0.6940445938786305,\"datasourceScores\":[{\"id\":\"genomics_england\",\"score\":0.865457038266544},{\"id\":\"eva\",\"score\":0.7224993021289249},{\"id\":\"gwas_credible_sets\",\"score\":0.6010782092453066},{\"id\":\"uniprot_literature\",\"score\":0.3039653988058105},{\"id\":\"chembl\",\"score\":0.14543899984528016},{\"id\":\"impc\",\"score\":0.5613216112390109},{\"id\":\"europepmc\",\"score\":0.20787604766308593}]},{\"target\":{\"id\":\"ENSG00000140718\",\"approvedSymbol\":\"FTO\"},\"score\":0.6347146523376355,\"datasourceScores\":[{\"id\":\"gwas_credible_sets\",\"score\":0.8779672796572509},{\"id\":\"uniprot_literature\",\"score\":0.607930797611621},{\"id\":\"impc\",\"score\":0.5466011344867336},{\"id\":\"europepmc\",\"score\":0.15685890443905623}]},{\"target\":{\"id\":\"ENSG00000138031\",\"approvedSymbol\":\"ADCY3\"},\"score\":0.6317017528023205,\"datasourceScores\":[{\"id\":\"uniprot_variants\",\"score\":0.7599134970145264},{\"id\":\"gwas_credible_sets\",\"score\":0.6830477233677074},{\"id\":\"uniprot_literature\",\"score\":0.607930797611621},{\"id\":\"eva\",\"score\":0.5984322886280611},{\"id\":\"impc\",\"score\":0.43451299866786125}]},{\"target\":{\"id\":\"ENSG00000118432\",\"approvedSymbol\":\"CNR1\"},\"score\":0.6163395558408818,\"datasourceScores\":[{\"id\":\"chembl\",\"score\":0.9305486691219842},{\"id\":\"uniprot_literature\",\"score\":0.3039653988058105},{\"id\":\"impc\",\"score\":0.2898006112214598},{\"id\":\"europepmc\",\"score\":0.06813891023230254}]},{\"target\":{\"id\":\"ENSG00000142319\",\"approvedSymbol\":\"SLC6A3\"},\"score\":0.6031451141525068,\"datasourceScores\":[{\"id\":\"chembl\",\"score\":0.9718921775895463},{\"id\":\"impc\",\"score\":0.4047147302399964}]},{\"target\":{\"id\":\"ENSG00000103546\",\"approvedSymbol\":\"SLC6A2\"},\"score\":0.59900800772549,\"datasourceScores\":[{\"id\":\"chembl\",\"score\":0.9853226881724267}]},{\"target\":{\"id\":\"ENSG00000176697\",\"approvedSymbol\":\"BDNF\"},\"score\":0.5967714745104006,\"datasourceScores\":[{\"id\":\"gwas_credible_sets\",\"score\":0.838978895964215},{\"id\":\"eva\",\"score\":0.50795995533771},{\"id\":\"impc\",\"score\":0.6007303713719272},{\"id\":\"europepmc\",\"score\":0.18602498028810888}]},{\"target\":{\"id\":\"ENSG00000175535\",\"approvedSymbol\":\"PNLIP\"},\"score\":0.5867868113870603,\"datasourceScores\":[{\"id\":\"chembl\",\"score\":0.9445060894135581},{\"id\":\"impc\",\"score\":0.40211455955771175},{\"id\":\"europepmc\",\"score\":0.02735688589252295}]},{\"target\":{\"id\":\"ENSG00000182333\",\"approvedSymbol\":\"LIPF\"},\"score\":0.5741943402862174,\"datasourceScores\":[{\"id\":\"chembl\",\"score\":0.9445060894135581}]},{\"target\":{\"id\":\"ENSG00000108576\",\"approvedSymbol\":\"SLC6A4\"},\"score\":0.5740260920575564,\"datasourceScores\":[{\"id\":\"chembl\",\"score\":0.943469420356006},{\"id\":\"europepmc\",\"score\":0.015198269940290528}]},{\"target\":{\"id\":\"ENSG00000112246\",\"approvedSymbol\":\"SIM1\"},\"score\":0.5712541862897182,\"datasourceScores\":[{\"id\":\"eva\",\"score\":0.6809060723483955},{\"id\":\"genomics_england\",\"score\":0.607930797611621},{\"id\":\"orphanet\",\"score\":0.607930797611621},{\"id\":\"gwas_credible_sets\",\"score\":0.5414986624935848},{\"id\":\"impc\",\"score\":0.6504622691606253},{\"id\":\"europepmc\",\"score\":0.03343619386863916}]},{\"target\":{\"id\":\"ENSG00000147246\",\"approvedSymbol\":\"HTR2C\"},\"score\":0.5679739766637659,\"datasourceScores\":[{\"id\":\"chembl\",\"score\":0.90996963738455},{\"id\":\"impc\",\"score\":0.48608852403158426}]},{\"target\":{\"id\":\"ENSG00000310517\",\"approvedSymbol\":\"CAST\"},\"score\":0.5625077037278304,\"datasourceScores\":[{\"id\":\"eva\",\"score\":0.896268691753036},{\"id\":\"gwas_credible_sets\",\"score\":0.11605507169815374}]},{\"target\":{\"id\":\"ENSG00000131910\",\"approvedSymbol\":\"NR0B2\"},\"score\":0.5595748682701085,\"datasourceScores\":[{\"id\":\"eva\",\"score\":0.7593088854360225},{\"id\":\"uniprot_literature\",\"score\":0.607930797611621},{\"id\":\"impc\",\"score\":0.34068441898155244},{\"id\":\"europepmc\",\"score\":0.12766546749844043}]},{\"target\":{\"id\":\"ENSG00000197594\",\"approvedSymbol\":\"ENPP1\"},\"score\":0.540870018475803,\"datasourceScores\":[{\"id\":\"eva\",\"score\":0.8857385602700697},{\"id\":\"europepmc\",\"score\":0.07903100368951074}]},{\"target\":{\"id\":\"ENSG00000244405\",\"approvedSymbol\":\"ETV5\"},\"score\":0.5333375345823379,\"datasourceScores\":[{\"id\":\"gwas_credible_sets\",\"score\":0.8635598349099746},{\"id\":\"impc\",\"score\":0.26128865681347474},{\"id\":\"europepmc\",\"score\":0.030396539880581056}]},{\"target\":{\"id\":\"ENSG00000120341\",\"approvedSymbol\":\"SEC16B\"},\"score\":0.5298617300299587,\"datasourceScores\":[{\"id\":\"gwas_credible_sets\",\"score\":0.8715823118546183}]},{\"target\":{\"id\":\"ENSG00000172260\",\"approvedSymbol\":\"NEGR1\"},\"score\":0.5290104025924977,\"datasourceScores\":[{\"id\":\"gwas_credible_sets\",\"score\":0.8671422888758696},{\"id\":\"europepmc\",\"score\":0.06079307976116211}]},{\"target\":{\"id\":\"ENSG00000148737\",\"approvedSymbol\":\"TCF7L2\"},\"score\":0.528903532248058,\"datasourceScores\":[{\"id\":\"gwas_credible_sets\",\"score\":0.8425456163029675},{\"id\":\"impc\",\"score\":0.5492106591870736}]},{\"target\":{\"id\":\"ENSG00000138821\",\"approvedSymbol\":\"SLC39A8\"},\"score\":0.5288922116793753,\"datasourceScores\":[{\"id\":\"gwas_credible_sets\",\"score\":0.8492913326674566},{\"id\":\"impc\",\"score\":0.4139239023797496}]},{\"target\":{\"id\":\"ENSG00000174007\",\"approvedSymbol\":\"CEP19\"},\"score\":0.5281423743048178,\"datasourceScores\":[{\"id\":\"orphanet\",\"score\":0.607930797611621},{\"id\":\"uniprot_literature\",\"score\":0.607930797611621},{\"id\":\"genomics_england\",\"score\":0.607930797611621},{\"id\":\"eva\",\"score\":0.5817357350212002},{\"id\":\"impc\",\"score\":0.5907032871791855},{\"id\":\"europepmc\",\"score\":0.037553236325797865}]},{\"target\":{\"id\":\"ENSG00000196353\",\"approvedSymbol\":\"CPNE4\"},\"score\":0.516923643051306,\"datasourceScores\":[{\"id\":\"gwas_credible_sets\",\"score\":0.8503001412038096}]},{\"target\":{\"id\":\"ENSG00000081138\",\"approvedSymbol\":\"CDH7\"},\"score\":0.5139414333929738,\"datasourceScores\":[{\"id\":\"gwas_credible_sets\",\"score\":0.8453946327642826}]},{\"target\":{\"id\":\"ENSG00000186487\",\"approvedSymbol\":\"MYT1L\"},\"score\":0.5126195988776844,\"datasourceScores\":[{\"id\":\"genomics_england\",\"score\":0.7599134970145264},{\"id\":\"gwas_credible_sets\",\"score\":0.3321465075099786},{\"id\":\"europepmc\",\"score\":0.012158615952232422}]},{\"target\":{\"id\":\"ENSG00000188778\",\"approvedSymbol\":\"ADRB3\"},\"score\":0.5125098223998064,\"datasourceScores\":[{\"id\":\"chembl\",\"score\":0.7366432570392815},{\"id\":\"eva\",\"score\":0.3742151798631534},{\"id\":\"europepmc\",\"score\":0.4191484956964723},{\"id\":\"impc\",\"score\":0.2822622693310757}]},{\"target\":{\"id\":\"ENSG00000053900\",\"approvedSymbol\":\"ANAPC4\"},\"score\":0.5074236781512301,\"datasourceScores\":[{\"id\":\"gwas_credible_sets\",\"score\":0.8346734203049863}]},{\"target\":{\"id\":\"ENSG00000136531\",\"approvedSymbol\":\"SCN2A\"},\"score\":0.5053710056344551,\"datasourceScores\":[{\"id\":\"chembl\",\"score\":0.6718896373478358},{\"id\":\"gwas_credible_sets\",\"score\":0.6356027341393482},{\"id\":\"europepmc\",\"score\":0.02279740491043579}]},{\"target\":{\"id\":\"ENSG00000010310\",\"approvedSymbol\":\"GIPR\"},\"score\":0.5018245175263728,\"datasourceScores\":[{\"id\":\"gwas_credible_sets\",\"score\":0.676529959085954},{\"id\":\"chembl\",\"score\":0.5648380254764791},{\"id\":\"impc\",\"score\":0.34756923526450406}]},{\"target\":{\"id\":\"ENSG00000153064\",\"approvedSymbol\":\"BANK1\"},\"score\":0.5017687601890164,\"datasourceScores\":[{\"id\":\"gwas_credible_sets\",\"score\":0.8253715096526058}]},{\"target\":{\"id\":\"ENSG00000008196\",\"approvedSymbol\":\"TFAP2B\"},\"score\":0.4989418973597177,\"datasourceScores\":[{\"id\":\"gwas_credible_sets\",\"score\":0.8207215349508723}]},{\"target\":{\"id\":\"ENSG00000130203\",\"approvedSymbol\":\"APOE\"},\"score\":0.4987521334557301,\"datasourceScores\":[{\"id\":\"gwas_credible_sets\",\"score\":0.7941628148016552},{\"id\":\"impc\",\"score\":0.46818379158574286},{\"id\":\"europepmc\",\"score\":0.1276822510754493}]},{\"target\":{\"id\":\"ENSG00000134443\",\"approvedSymbol\":\"GRP\"},\"score\":0.4981005284868444,\"datasourceScores\":[{\"id\":\"gwas_credible_sets\",\"score\":0.8193375470427439}]},{\"target\":{\"id\":\"ENSG00000155393\",\"approvedSymbol\":\"HEATR3\"},\"score\":0.4940564029639243,\"datasourceScores\":[{\"id\":\"genomics_england\",\"score\":0.8126852676405352}]},{\"target\":{\"id\":\"ENSG00000150594\",\"approvedSymbol\":\"ADRA2A\"},\"score\":0.4927764444047516,\"datasourceScores\":[{\"id\":\"chembl\",\"score\":0.7979105551796934},{\"id\":\"impc\",\"score\":0.25338555644452365}]},{\"target\":{\"id\":\"ENSG00000274286\",\"approvedSymbol\":\"ADRA2B\"},\"score\":0.49068506303919535,\"datasourceScores\":[{\"id\":\"chembl\",\"score\":0.7979105551796934},{\"id\":\"gwas_credible_sets\",\"score\":0.03691645712384059}]},{\"target\":{\"id\":\"ENSG00000158321\",\"approvedSymbol\":\"AUTS2\"},\"score\":0.48994628509802307,\"datasourceScores\":[{\"id\":\"gwas_credible_sets\",\"score\":0.8059244358451259}]},{\"target\":{\"id\":\"ENSG00000184160\",\"approvedSymbol\":\"ADRA2C\"},\"score\":0.4850744002331224,\"datasourceScores\":[{\"id\":\"chembl\",\"score\":0.7979105551796934}]},{\"target\":{\"id\":\"ENSG00000164574\",\"approvedSymbol\":\"GALNT10\"},\"score\":0.48361599403442934,\"datasourceScores\":[{\"id\":\"gwas_credible_sets\",\"score\":0.7955115877241496}]},{\"target\":{\"id\":\"ENSG00000189283\",\"approvedSymbol\":\"FHIT\"},\"score\":0.4818539941319322,\"datasourceScores\":[{\"id\":\"gwas_credible_sets\",\"score\":0.7926132316786597}]},{\"target\":{\"id\":\"ENSG00000155511\",\"approvedSymbol\":\"GRIA1\"},\"score\":0.4813092428966535,\"datasourceScores\":[{\"id\":\"chembl\",\"score\":0.6635780821853672},{\"id\":\"gwas_credible_sets\",\"score\":0.5125563003036403}]},{\"target\":{\"id\":\"ENSG00000196616\",\"approvedSymbol\":\"ADH1B\"},\"score\":0.48049875521861024,\"datasourceScores\":[{\"id\":\"gwas_credible_sets\",\"score\":0.7903839665737394}]},{\"target\":{\"id\":\"ENSG00000152359\",\"approvedSymbol\":\"POC5\"},\"score\":0.4798059176121916,\"datasourceScores\":[{\"id\":\"gwas_credible_sets\",\"score\":0.7892443013204892}]},{\"target\":{\"id\":\"ENSG00000144285\",\"approvedSymbol\":\"SCN1A\"},\"score\":0.47498677698309894,\"datasourceScores\":[{\"id\":\"chembl\",\"score\":0.6718896373478358},{\"id\":\"eva\",\"score\":0.4377101742803672}]},{\"target\":{\"id\":\"ENSG00000174482\",\"approvedSymbol\":\"LINGO2\"},\"score\":0.47267428208217677,\"datasourceScores\":[{\"id\":\"gwas_credible_sets\",\"score\":0.7775133024008212}]}]}}}}\n",
      "  target     score\n",
      "0   MC4R  0.831735\n",
      "1  PCSK1  0.782166\n",
      "2   LEPR  0.758118\n",
      "3   POMC  0.752802\n",
      "4  GLP1R  0.724787\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Europe PMC API: Search for recent anti-obesity articles\n",
    "def fetch_epmc_articles(query, from_year=2024, to_year=2025, max_results=1000):\n",
    "    url = \"https://www.ebi.ac.uk/europepmc/webservices/rest/search\"\n",
    "    params = {\n",
    "        'query': f'{query} AND PUB_YEAR:[{from_year} TO {to_year}]',\n",
    "        'format': 'json',\n",
    "        'pageSize': max_results,\n",
    "        'resultType': 'core'\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.ok:\n",
    "        results = response.json()\n",
    "        articles = results.get('resultList', {}).get('result', [])\n",
    "        #for art in articles[:5]:\n",
    "        #    print(\"Article keys:\", list(art.keys()))\n",
    "        # Extract titles and abstracts\n",
    "        data = [{ \n",
    "                'title': art.get('title', ''), \n",
    "                'abstract': art.get('abstractText', art.get('abstract', '')), \n",
    "                'pubYear': art.get('pubYear', '')\n",
    "            } for art in articles]\n",
    "        return pd.DataFrame(data)\n",
    "    else:\n",
    "        print(\"Europe PMC request failed:\", response.status_code)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Open Targets Platform API: Get latest targets for obesity\n",
    "def fetch_opentargets_targets(disease_efo=\"EFO_0001073\", size=50):\n",
    "    # EFO_0001073 is the code for \"obesity\"\n",
    "    url = \"https://api.platform.opentargets.org/api/v4/graphql\"\n",
    "    query = \"\"\"\n",
    "    query TargetSearch($efoId: String!, $size: Int!) {\n",
    "      disease(efoId:$efoId){\n",
    "          id\n",
    "          name\n",
    "          associatedTargets(page: {size: $size, index: 0}){\n",
    "              count\n",
    "              rows{\n",
    "                  target{\n",
    "                      id\n",
    "                      approvedSymbol\n",
    "                  }\n",
    "                  score\n",
    "                  datasourceScores{\n",
    "                      id\n",
    "                      score\n",
    "                  }\n",
    "              }\n",
    "          }\n",
    "      }\n",
    "    }  \n",
    "    \"\"\"\n",
    "    variables = {\"efoId\": disease_efo, \"size\": size}\n",
    "    response = requests.post(url, json={\"query\": query, \"variables\": variables})\n",
    "    print(response.status_code)\n",
    "    print(response.text)\n",
    "\n",
    "    if response.ok:\n",
    "        results = response.json()\n",
    "        data = []\n",
    "        for item in results['data']['disease']['associatedTargets']['rows']:\n",
    "            gene = item['target']['approvedSymbol']\n",
    "            score = item['score']\n",
    "            data.append({'target': gene, 'score': score})\n",
    "        return pd.DataFrame(data)\n",
    "    else:\n",
    "        print(\"Open Targets request failed:\", response.status_code)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Example usage:\n",
    "# Fetch literature\n",
    "df_articles = fetch_epmc_articles(\"obesity target\", 2023, 2025)\n",
    "print(df_articles.head())\n",
    "\n",
    "# Fetch targets\n",
    "df_targets = fetch_opentargets_targets()\n",
    "print(df_targets.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e084177",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('insulin', 378), ('leptin', 93), ('glp - 1', 57), ('ampk', 54), ('g', 47), ('fto', 46), ('h', 45), ('hba1c', 43), ('glp - 1 ras', 35), ('sirt1', 33), ('pparγ', 32), ('ucp1', 32), ('il - 6', 31), ('s', 30), ('mtor', 28), ('glucagon - like peptide - 1 receptor', 28), ('adiponectin', 28), ('glp - 1 receptor', 28), ('tnf - α', 28), ('akt', 26)]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "def chunk_text(text, tokenizer, max_length=510):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), max_length):\n",
    "        chunk_tokens = tokens[i:i + max_length]\n",
    "        chunk = tokenizer.convert_tokens_to_string(chunk_tokens)\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "# Step 1: Load BioBERT model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"alvaroalon2/biobert_genetic_ner\")\n",
    "nlp_ner = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "# Step 2: Run NER on your abstracts/titles\n",
    "def extract_targets_biobert(text_list, top_n=20):\n",
    "    entity_freq = {}\n",
    "    for text in text_list:\n",
    "        token_count = len(tokenizer.tokenize(text))\n",
    "        if token_count > 510:\n",
    "            text_chunks = chunk_text(text, tokenizer)\n",
    "        else:\n",
    "            text_chunks = [text]\n",
    "        for chunk in text_chunks:\n",
    "            ner_results = nlp_ner(chunk)\n",
    "            for entity in ner_results:\n",
    "                # Check your model's field name! If it uses entity['entity'], use that\n",
    "                ent_grp = entity.get('entity_group', entity.get('entity', ''))\n",
    "                if ent_grp in ['GENE', 'PROTEIN','GENETIC']:\n",
    "                    ent = entity['word']\n",
    "                    entity_freq[ent] = entity_freq.get(ent, 0) + 1\n",
    "    sorted_entities = sorted(entity_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_entities[:top_n]\n",
    "\n",
    "# Example usage:\n",
    "top_targets = extract_targets_biobert(df_articles[\"abstract\"].dropna().tolist())\n",
    "print(top_targets)\n",
    "df_articles['targets'] = str(top_targets)\n",
    "df_articles.to_csv(\"output.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ffb8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt using BERN2 for extraction and normalization\n",
    "#  References:\n",
    "#  - BERN2:https://github.com/sudha-vijayakumar/BERN2_TigerGraph_BioMedical_KnowledgeGraph/blob/main/bioNLP%202/1_bioNLP_Data.ipynb\n",
    "#  https://medium.com/@sudha.vijayakumar_74093/implementing-a-biomedical-knowledge-graph-using-bern2-and-tigergraph-56a5e670782a\n",
    "# There are many other models available at https://huggingface.co/models?search=dmis-lab but chief among them is BioBERT. BERN2 also does normalization.\n",
    "# and was created as a NER + normalization tool over BioBERT. BioBERT has independently been improved, so we will try that out separately as well.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca8f7de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_biobert(text):\n",
    "    # Handles long texts by chunking\n",
    "    token_count = len(tokenizer.tokenize(text))\n",
    "    if token_count > 510:\n",
    "        text_chunks = chunk_text(text, tokenizer)\n",
    "    else:\n",
    "        text_chunks = [text]\n",
    "    entities = []\n",
    "    for chunk in text_chunks:\n",
    "        ner_results = nlp_ner(chunk)\n",
    "        for entity in ner_results:\n",
    "            ent_grp = entity.get('entity_group', entity.get('entity', ''))\n",
    "            if ent_grp in ['GENE', 'PROTEIN', 'GENETIC']:\n",
    "                entities.append(entity['word'])\n",
    "    # Optionally, remove duplicates by converting to set: list(set(entities))  \n",
    "    return entities\n",
    "\n",
    "# Apply to dataframe\n",
    "df_articles['targets'] = df_articles['abstract'].fillna('').apply(extract_entities_biobert)\n",
    "\n",
    "# Save to CSV\n",
    "df_articles.to_csv(\"output.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b3707de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement numpy==1.26.4 (from versions: 2.1.0, 2.1.1, 2.1.2, 2.1.3, 2.2.0, 2.2.1, 2.2.2, 2.2.3, 2.2.4, 2.2.5, 2.2.6, 2.3.0, 2.3.1, 2.3.2, 2.3.3)\n",
      "ERROR: No matching distribution found for numpy==1.26.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\aksha\\onedrive\\desktop\\education courses\\iitk ai ml\\github\\psp\\.venv\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\aksha\\onedrive\\desktop\\education courses\\iitk ai ml\\github\\psp\\.venv\\lib\\site-packages (from scikit-learn) (2.3.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\aksha\\onedrive\\desktop\\education courses\\iitk ai ml\\github\\psp\\.venv\\lib\\site-packages (from scikit-learn) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\aksha\\onedrive\\desktop\\education courses\\iitk ai ml\\github\\psp\\.venv\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\aksha\\onedrive\\desktop\\education courses\\iitk ai ml\\github\\psp\\.venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Collecting gilda\n",
      "  Using cached gilda-1.4.1-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: regex in c:\\users\\aksha\\onedrive\\desktop\\education courses\\iitk ai ml\\github\\psp\\.venv\\lib\\site-packages (from gilda) (2025.9.18)\n",
      "Collecting boto3 (from gilda)\n",
      "  Using cached boto3-1.40.44-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting adeft (from gilda)\n",
      "  Using cached adeft-0.12.3.tar.gz (177 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting flask<4.0,>=3.0 (from gilda)\n",
      "  Using cached flask-3.1.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting flask-restx>=1.3.0 (from gilda)\n",
      "  Using cached flask_restx-1.3.2-py2.py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting pystow>=0.1.10 (from gilda)\n",
      "  Using cached pystow-0.7.11-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting unidecode (from gilda)\n",
      "  Using cached Unidecode-1.4.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting werkzeug (from gilda)\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting scikit-learn<1.5 (from gilda)\n",
      "  Using cached scikit-learn-1.4.2.tar.gz (7.8 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × pip subprocess to install build dependencies did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [9 lines of output]\n",
      "      Collecting setuptools\n",
      "        Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "      Collecting wheel\n",
      "        Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "      Collecting Cython>=3.0.8\n",
      "        Using cached cython-3.1.4-cp313-cp313-win_amd64.whl.metadata (5.1 kB)\n",
      "      ERROR: Ignored the following versions that require a different python version: 1.21.2 Requires-Python >=3.7,<3.11; 1.21.3 Requires-Python >=3.7,<3.11; 1.21.4 Requires-Python >=3.7,<3.11; 1.21.5 Requires-Python >=3.7,<3.11; 1.21.6 Requires-Python >=3.7,<3.11; 1.26.0 Requires-Python >=3.9,<3.13; 1.26.1 Requires-Python >=3.9,<3.13\n",
      "      ERROR: Could not find a version that satisfies the requirement numpy==2.0.0rc1 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 1.13.3, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5, 1.20.0, 1.20.1, 1.20.2, 1.20.3, 1.21.0, 1.21.1, 1.22.0, 1.22.1, 1.22.2, 1.22.3, 1.22.4, 1.23.0, 1.23.1, 1.23.2, 1.23.3, 1.23.4, 1.23.5, 1.24.0, 1.24.1, 1.24.2, 1.24.3, 1.24.4, 1.25.0, 1.25.1, 1.25.2, 1.26.2, 1.26.3, 1.26.4, 2.0.0, 2.0.1, 2.0.2, 2.1.0, 2.1.1, 2.1.2, 2.1.3, 2.2.0, 2.2.1, 2.2.2, 2.2.3, 2.2.4, 2.2.5, 2.2.6, 2.3.0, 2.3.1, 2.3.2, 2.3.3)\n",
      "      ERROR: No matching distribution found for numpy==2.0.0rc1\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "× pip subprocess to install build dependencies did not run successfully.\n",
      "│ exit code: 1\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall numpy\n",
    "!pip install numpy==1.26.4 --only-binary=:all:\n",
    "!pip install scikit-learn\n",
    "!pip install gilda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a74ab9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gilda\n",
    "import re\n",
    "\n",
    "def extract_term_label(term_obj):\n",
    "    term_str = repr(term_obj)\n",
    "    m = re.match(r\"Term\\(([^,]+)\", term_str)\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def normalize_entity_list(entity_list):\n",
    "    normalized_names = []\n",
    "    if not isinstance(entity_list, list):\n",
    "        return normalized_names  # empty if input is not a list\n",
    "    for term in entity_list:\n",
    "        norm = gilda.ground(term)\n",
    "        if norm:\n",
    "            best = norm[0]\n",
    "            n_label = extract_term_label(best.term)\n",
    "            if n_label and n_label not in normalized_names and len(n_label) > 2:\n",
    "                normalized_names.append(n_label)\n",
    "    return normalized_names\n",
    "\n",
    "# Add normalized_text column (list of normalized names for each abstract)\n",
    "df_articles['normalized_text'] = df_articles['targets'].apply(normalize_entity_list)\n",
    "\n",
    "# Save as CSV\n",
    "df_articles.to_csv(\"output_with_normalized.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e890a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('insulin', 186), ('glucagon like peptide 1 receptor', 32), ('glp 1', 28), ('hba1c', 27), ('ampk', 26), ('leptin', 26), ('glp 1 receptor', 23), ('glucagon like peptide 1', 22), ('mtor', 19), ('pparγ', 19), ('ucp1', 19), ('akt', 18), ('pi3k', 15), ('uncoupling protein 1', 15), ('adiponectin', 14), ('sirt1', 12), ('gip', 12), ('incretin', 10), ('stat3', 9), ('amp activated protein kinase', 9), ('interleukin 6', 9), ('hba1', 8), ('renin', 8), ('protein kinase b', 7), ('nrf2', 7), ('pancreatic lipase', 7), ('ghrelin', 7), ('fgf21', 7), ('pparg', 6), ('fasn', 6), ('nlrp3', 6), ('g protein coupled receptors', 6), ('fto', 6), ('egfr', 6), ('low density lipoprotein', 6), ('mtorc1', 6), ('p38', 6), ('tnf', 5), ('fatty acid synthase', 5), ('tlr4', 5), ('esr1', 5), ('insulin like growth factor 1', 5), ('histone', 5), ('ppar', 5), ('glucagon receptor', 5), ('ldl', 5), ('glucagon', 5), ('c reactive protein', 5), ('angiotensin', 5), ('mc4r', 5), ('jnk', 5), ('gipr', 5), ('sglt2', 5), ('hemoglobin', 5), ('cd36', 4), ('gip receptor', 4), ('rbp', 4), ('amylin', 4), ('pro opiomelanocortin', 4), ('dipeptidyl peptidase 4', 4), ('erk', 4), ('irisin', 4), ('crispr', 4), ('atgl', 4), ('pparalpha', 4), ('foxo1', 4), ('albumin', 4), ('c peptide', 4), ('glucose dependent insulinotropic peptide', 4), ('interleukin 1 beta', 4), ('tumor necrosis factor alpha', 4), ('dnmt3a', 4), ('wnt', 4), ('crp', 4), ('glp1r', 4), ('pka', 4), ('nampt', 4), ('hemoglobin a1c', 4), ('fabp4', 4), ('adipoq', 4), ('leptin receptor', 4), ('lepr', 4), ('fxr', 4), ('hmg coa reductase', 3), ('il6', 3), ('akt1', 3), ('high density lipoprotein', 3), ('mammalian target of rapamycin', 3), ('adpn', 3), ('low density lipoprotein cholesterol', 3), ('mapk', 3), ('hur', 3), ('luciferase', 3), ('interleukin 18', 3), ('calcitonin receptor', 3), ('neuropeptide y', 3), ('agrp', 3), ('pomc', 3), ('m6a', 3), ('chemerin', 3), ('p53', 3), ('fas', 3), ('tp53', 3), ('tumor necrosis factor', 3), ('endothelial nitric oxide synthase', 3), ('enos', 3), ('sirtuin 1', 3), ('peroxisome proliferator activated receptor gamma coactivator 1 alpha', 3), ('rna binding proteins', 3), ('vldl', 3), ('phosphatidylinositol 3 kinase', 3), ('il1b', 3), ('jak', 3), ('sirtuin', 3), ('tgr5', 3), ('glp', 3), ('foxo', 3), ('ferritin', 3), ('mash', 3), ('vegf', 3), ('peroxisome proliferator activated receptor gamma', 3), ('lpl', 3), ('pcsk9', 3), ('ccl5', 3), ('mc3r', 3), ('mineralocorticoid receptor', 3), ('prdm16', 3), ('brain derived neurotrophic factor', 3), ('bdnf', 3), ('alt', 3), ('statin', 3), ('gpcr', 3), ('trem2', 3), ('hdl', 3), ('phosphoinositide 3 kinase', 2), ('ppara', 2), ('pik3ca', 2), ('insr', 2), ('lipase', 2), ('hsl', 2), ('hem', 2), ('lactate dehydrogenase', 2), ('creb', 2), ('nox2', 2), ('ins', 2), ('lep', 2), ('ras', 2), ('orexin', 2), ('amylin receptor', 2), ('trpc3', 2), ('npy', 2), ('myd88', 2), ('hsp90aa1', 2), ('alpha glucosidase', 2), ('ang ii', 2), ('acadm', 2), ('tnfr1', 2), ('myostatin', 2), ('adipose triglyceride lipase', 2), ('heme oxygenase 1', 2), ('glut4', 2), ('protein disulfide isomerase', 2), ('pdi', 2), ('gsk3β', 2), ('perk', 2), ('ppargc1a', 2), ('egf', 2), ('estrogen receptor alpha', 2), ('dna methyltransferase 1', 2), ('dnmt1', 2), ('toll like receptor 4', 2), ('sirt', 2), ('glut2', 2), ('ccr5', 2), ('foxp3', 2), ('runx1', 2), ('tlr8', 2), ('cd68', 2), ('fibroblast growth factor 21', 2), ('e3 ubiquitin ligase', 2), ('ap2', 2), ('apoa1', 2), ('drp1', 2), ('amylase', 2), ('epo', 2), ('g protein coupled receptor', 2), ('protein kinase a', 2), ('nicotinamide phosphoribosyltransferase', 2), ('ror1', 2), ('gata6', 2), ('bmp2', 2), ('tub', 2), ('g protein', 2), ('apob', 2), ('ldl cholesterol', 2), ('histone deacetylase', 2), ('s100a8', 2), ('s100a9', 2), ('sglt1', 2), ('cyp3a4', 2), ('interleukin 15', 2), ('gsh', 2), ('ccl3', 2), ('cxcl2', 2), ('arc', 2), ('thyroid hormone receptor beta', 2), ('gastric inhibitory polypeptide receptor', 2), ('lc3', 2), ('interleukin', 2), ('fgf', 2), ('bmp', 2), ('coxiv', 2), ('alb', 2), ('gcgr', 2), ('melanocortin 4 receptor', 2), ('apolipoprotein b', 2), ('incretins', 2), ('target of rapamycin complex 1', 2), ('cgas', 2), ('irf3', 2), ('insulin receptor substrate 1', 2), ('protein tyrosine phosphatases', 2), ('abpp', 2), ('ucp 1', 2), ('tgf β', 2), ('thyroid hormone receptor', 2), ('ast', 2), ('alp', 2), ('insulin receptor', 2), ('rankl', 2), ('mhc', 2), ('fsh', 2), ('hif', 2), ('srebp1', 2), ('cert', 1), ('cer', 1), ('pik3r1', 1), ('ppi', 1), ('hk1', 1), ('hk2', 1), ('acacb', 1), ('slc2a4', 1), ('acaca', 1), ('isoquercetin', 1), ('karanjin', 1), ('scfa', 1), ('cd44', 1), ('eosin', 1), ('angiotensin converting enzyme 2', 1), ('ace2', 1), ('asct2', 1), ('lat1', 1), ('xct', 1), ('fatty acid transport protein 2', 1), ('fatp2', 1), ('membrane type 1 matrix metalloproteinase', 1), ('mmp14', 1), ('gpr158', 1), ('osteocalcin', 1), ('ocn', 1), ('gprc5b', 1), ('camp response element binding protein', 1), ('mafa', 1), ('gcn5', 1), ('cpth2', 1), ('cpt1', 1), ('acox1', 1), ('glucagon like peptide 2', 1), ('snhg16', 1), ('rab10', 1), ('dr1', 1), ('ptbp3', 1), ('ncbp1', 1), ('clcn4', 1), ('plcb1', 1), ('cdc25b', 1), ('erbb4', 1), ('tnrc6c', 1), ('serpine1', 1), ('orexin a', 1), ('orexin b', 1), ('mor', 1), ('hur protein', 1), ('calcitonin', 1), ('receptor activity modifying protein', 1), ('amylin receptors', 1), ('ww domain containing e3 ubiquitin protein ligase 1', 1), ('wwp1', 1), ('notch', 1), ('cmklr1', 1), ('gpr1', 1), ('ccrl2', 1), ('lgals3', 1), ('ccnd1', 1), ('bcl2', 1), ('src', 1), ('monoamine oxidase', 1), ('mao', 1), ('reggamma', 1), ('klf15', 1), ('cxcl5', 1), ('cd8', 1), ('programmed cell death protein 1', 1), ('thyroid stimulating hormone', 1), ('tsh', 1), ('rhamm', 1), ('nqo1', 1), ('gsk3', 1), ('arginase', 1), ('arg', 1), ('col5a3', 1), ('ctr', 1), ('cln3', 1), ('sh2b1', 1), ('atp2a1', 1), ('mmel1', 1), ('igd', 1), ('atglistatin', 1), ('il 13', 1), ('stat6', 1), ('ahnak', 1), ('rap2a', 1), ('peroxisome proliferator activated receptors', 1), ('crt', 1), ('protein kinase d', 1), ('pkd', 1), ('fetuin a', 1), ('feta', 1), ('prolyl hydroxylase', 1), ('glp1', 1), ('mitochondrial carrier homolog 2', 1), ('carnitine palmitoyltransferase i', 1), ('shank3', 1), ('socs1', 1), ('ptb', 1), ('ythdf1', 1), ('leap2', 1), ('ghrelin receptor', 1), ('ceacam1', 1), ('proinsulin', 1), ('clathrin', 1), ('dynamin', 1), ('rab5', 1), ('rab7', 1), ('pdk1', 1), ('ire1alpha', 1), ('xbp1', 1), ('eif2alpha', 1), ('atf6', 1), ('nafld', 1), ('prolactin releasing peptide', 1), ('pyy', 1), ('glicentin', 1), ('oxyntomodulin', 1), ('gckr', 1), ('cyb5a', 1), ('itpka', 1), ('entpd6', 1), ('sema3g', 1), ('foxo3', 1), ('hapln4', 1), ('shh', 1), ('dmf', 1), ('camkk2', 1), ('itga2b', 1), ('pip4k2a', 1), ('hsp47', 1), ('collagen', 1), ('asporin', 1), ('janus kinase', 1), ('stat', 1), ('glucose transporter type 2', 1), ('gpr43', 1), ('trpm7', 1), ('csn', 1), ('dio', 1), ('cre recombinase', 1), ('green fluorescent protein', 1), ('fty720', 1), ('interferon regulatory factor 5', 1), ('irf5', 1), ('cd11c', 1), ('cd16', 1), ('cd163', 1), ('irak1', 1), ('ccl7', 1), ('cxcl11', 1), ('tlr2', 1), ('tlr9', 1), ('cd86', 1), ('irf4', 1), ('for', 1), ('iodothyronine deiodinase 2', 1), ('lgr4', 1), ('rspo2', 1), ('mfc', 1), ('ret finger protein', 1), ('rfp', 1), ('trim27', 1), ('asprosin', 1), ('asp', 1), ('parathyroid hormone', 1), ('ffa', 1), ('mettl3', 1), ('apoc1', 1), ('apod', 1), ('ttr', 1), ('fgb', 1), ('fgg', 1), ('il1', 1), ('cordycepin', 1), ('cpn', 1), ('cps1', 1), ('hras', 1), ('mapk14', 1), ('pah', 1), ('aldob', 1), ('gsk3b', 1), ('bhmt2', 1), ('casp3', 1), ('mat1a', 1), ('apom', 1), ('apoa2', 1), ('apoc3', 1), ('nuclear receptor subfamily 4 group a member 1', 1), ('nr4a1', 1), ('tipe2', 1), ('cyclooxygenase 2', 1), ('cox2', 1), ('hepcidin', 1), ('ferroportin', 1), ('thioredoxin interacting protein', 1), ('txnip', 1), ('celf1', 1), ('erythropoietin', 1), ('micb', 1), ('c6orf15', 1), ('nachr', 1), ('nicotinic receptor', 1), ('glycosylated hemoglobin', 1), ('adenylate cyclase', 1), ('epac', 1), ('trpv1', 1), ('pepck', 1), ('serca', 1), ('sarcolipin', 1), ('end', 1), ('mkrn1', 1), ('mef2d', 1), ('ctrb2', 1), ('rspo3', 1), ('klotho beta', 1), ('receptor tyrosine kinase like orphan receptor 1', 1), ('retinoid x receptor gamma', 1), ('rxrgamma', 1), ('pla2', 1), ('spla2', 1), ('spla2s', 1), ('mll4', 1), ('kmt2d', 1), ('histone lysine methyltransferase', 1), ('h3k4me1', 1), ('h3k27ac', 1), ('fatty acid binding protein 4', 1), ('somatostatin', 1), ('snhg9', 1), ('toll like receptors', 1), ('abc transporter', 1), ('igf', 1), ('vegfr', 1), ('smyd3', 1), ('transcription', 1), ('abhd6', 1), ('masld', 1), ('adrb3', 1), ('pka calpha', 1), ('pregnane x receptor', 1), ('pxr', 1), ('nr1i2', 1), ('grn', 1), ('lilrb5', 1), ('cr1', 1), ('tnfsf12', 1), ('dapk2', 1), ('cpg', 1), ('aml1', 1), ('klhdc3', 1), ('hint1', 1), ('hspb1', 1), ('serum', 1), ('apo', 1), ('ras responsive element binding protein 1', 1), ('rreb1', 1), ('tyrosine kinase', 1), ('tbx1', 1), ('ucp1 protein', 1), ('ptpn1', 1), ('jak2', 1), ('trim59', 1), ('bax', 1)]\n"
     ]
    }
   ],
   "source": [
    "def extract_targets_biobert(ent_list, top_n=20):\n",
    "    entity_freq = {}\n",
    "    for ent in ent_list:\n",
    "        entity_freq[ent] = entity_freq.get(ent, 0) + 1\n",
    "    sorted_entities = sorted(entity_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_entities[:top_n]\n",
    "\n",
    "print(extract_targets_biobert(df_articles['normalized_text'].explode().dropna().tolist(), top_n=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b54fd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "c:\\Users\\aksha\\anaconda3\\envs\\psp\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\aksha\\.cache\\huggingface\\hub\\models--dmis-lab--bern2-ner. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING: [2025-10-06 12:00:37] huggingface_hub.file_download - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING: [2025-10-06 12:08:27] huggingface_hub.file_download - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cpu\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at dmis-lab/bern2-ner and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n",
      "c:\\Users\\aksha\\anaconda3\\envs\\psp\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\aksha/nltk_data'\n    - 'c:\\\\Users\\\\aksha\\\\anaconda3\\\\envs\\\\psp\\\\nltk_data'\n    - 'c:\\\\Users\\\\aksha\\\\anaconda3\\\\envs\\\\psp\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\aksha\\\\anaconda3\\\\envs\\\\psp\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\aksha\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 84\u001b[39m\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m evidence\n\u001b[32m     83\u001b[39m \u001b[38;5;66;03m# Example\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m evidence = \u001b[43maggregate_relations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_articles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_score\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m# Summarize by target\u001b[39;00m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 77\u001b[39m, in \u001b[36maggregate_relations\u001b[39m\u001b[34m(df_articles, min_score)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m df_articles.dropna(subset=[\u001b[33m\"\u001b[39m\u001b[33mabstract\u001b[39m\u001b[33m\"\u001b[39m]).iterrows():\n\u001b[32m     76\u001b[39m     abs_text = row[\u001b[33m\"\u001b[39m\u001b[33mabstract\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     rels = \u001b[43mextract_target_relations_from_abstract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mabs_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m tgt, sent, score \u001b[38;5;129;01min\u001b[39;00m rels:\n\u001b[32m     79\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m score >= min_score:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 60\u001b[39m, in \u001b[36mextract_target_relations_from_abstract\u001b[39m\u001b[34m(abstract)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract_target_relations_from_abstract\u001b[39m(abstract: \u001b[38;5;28mstr\u001b[39m) -> List[Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[32m     59\u001b[39m     relations = []\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mabstract\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     61\u001b[39m         targets = extract_targets_from_sentence(sent)\n\u001b[32m     62\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m targets:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aksha\\anaconda3\\envs\\psp\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aksha\\anaconda3\\envs\\psp\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aksha\\anaconda3\\envs\\psp\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aksha\\anaconda3\\envs\\psp\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aksha\\anaconda3\\envs\\psp\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\aksha/nltk_data'\n    - 'c:\\\\Users\\\\aksha\\\\anaconda3\\\\envs\\\\psp\\\\nltk_data'\n    - 'c:\\\\Users\\\\aksha\\\\anaconda3\\\\envs\\\\psp\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\aksha\\\\anaconda3\\\\envs\\\\psp\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\aksha\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aksha\\anaconda3\\envs\\psp\\Lib\\site-packages\\huggingface_hub\\file_download.py:801: UserWarning: Not enough free disk space to download the file. The expected file size is: 1458.35 MB. The target location C:\\Users\\aksha\\.cache\\huggingface\\hub\\models--dmis-lab--bern2-ner\\blobs only has 856.94 MB free disk space.\n",
      "  warnings.warn(\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING: [2025-10-06 12:14:47] huggingface_hub.file_download - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import List, Tuple, Dict\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import nltk\n",
    "\n",
    "# 1) Sentence splitter\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# 2) BioBERT NER\n",
    "ner_tok = AutoTokenizer.from_pretrained(\"dmis-lab/bern2-ner\")\n",
    "ner_model = AutoModelForTokenClassification.from_pretrained(\"dmis-lab/bern2-ner\")\n",
    "ner = pipeline(\"ner\", model=ner_model, tokenizer=ner_tok, aggregation_strategy=\"simple\", device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "# 3) Biomedical Relation Classifier (binary: relation vs no relation)\n",
    "# Replace with a relation model you prefer; here we assume a generic biomed RE binary classifier\n",
    "re_model_name = \"dmis-lab/bern2-ner\"  # example placeholder; choose a binary RE model for your use case\n",
    "re_tok = AutoTokenizer.from_pretrained(re_model_name)\n",
    "re_model = AutoModelForSequenceClassification.from_pretrained(re_model_name)\n",
    "re_pipe = pipeline(\"text-classification\", model=re_model, tokenizer=re_tok, return_all_scores=True, device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "# Helper: extract candidate targets (GENE/PROTEIN) from sentence\n",
    "def extract_targets_from_sentence(sentence: str) -> List[str]:\n",
    "    ents = ner(sentence)\n",
    "    targets = []\n",
    "    for e in ents:\n",
    "        if e[\"entity_group\"] in [\"GENE\", \"PROTEIN\"]:\n",
    "            targets.append(e[\"word\"])\n",
    "    # deduplicate\n",
    "    return list(dict.fromkeys([t.strip() for t in targets]))\n",
    "\n",
    "# Build RE input: we’ll format as \"[SUBJ] {target} [/SUBJ] ... [OBJ] obesity [/OBJ]\"\n",
    "def make_re_text(sentence: str, subj: str, obj: str = \"obesity\") -> str:\n",
    "    # simple insertion: highlight mentions; if multiple occurrences, mark the first occurrence\n",
    "    s = sentence\n",
    "    # case-insensitive mark of subject\n",
    "    pat_subj = re.compile(re.escape(subj), re.IGNORECASE)\n",
    "    s = pat_subj.sub(f\"[SUBJ]{subj}[/SUBJ]\", s, count=1)\n",
    "    # mark object \"obesity\"\n",
    "    pat_obj = re.compile(r\"\\bobesity\\b\", re.IGNORECASE)\n",
    "    s = pat_obj.sub(\"[OBJ]obesity[/OBJ]\", s, count=1)\n",
    "    return s\n",
    "\n",
    "# Score relation per (target, sentence)\n",
    "def score_relation(sentence: str, target: str) -> float:\n",
    "    # require \"obesity\" to be present to attempt RE\n",
    "    if not re.search(r\"\\bobesity\\b\", sentence, re.IGNORECASE):\n",
    "        return 0.0\n",
    "    re_text = make_re_text(sentence, target, \"obesity\")\n",
    "    scores = re_pipe(re_text)[0]   # list of dicts with 'label' and 'score'\n",
    "    # Map to a positive relation score; adjust labels depending on model\n",
    "    # Example assumption: labels ['NEGATIVE','POSITIVE']\n",
    "    label_scores = {d['label'].upper(): d['score'] for d in scores}\n",
    "    pos_score = label_scores.get('POSITIVE', 0.0)\n",
    "    return float(pos_score)\n",
    "\n",
    "def extract_target_relations_from_abstract(abstract: str) -> List[Tuple[str, str, float]]:\n",
    "    relations = []\n",
    "    for sent in sent_tokenize(abstract):\n",
    "        targets = extract_targets_from_sentence(sent)\n",
    "        if not targets:\n",
    "            continue\n",
    "        if not re.search(r\"\\bobesity\\b\", sent, re.IGNORECASE):\n",
    "            continue\n",
    "        for tgt in targets:\n",
    "            score = score_relation(sent, tgt)\n",
    "            if score > 0:  # keep only positive evidence\n",
    "                relations.append((tgt.upper(), sent, score))\n",
    "    return relations\n",
    "\n",
    "# Aggregate over your dataframe df_articles\n",
    "def aggregate_relations(df_articles, min_score=0.5):\n",
    "    evidence = []\n",
    "    for idx, row in df_articles.dropna(subset=[\"abstract\"]).iterrows():\n",
    "        abs_text = row[\"abstract\"]\n",
    "        rels = extract_target_relations_from_abstract(abs_text)\n",
    "        for tgt, sent, score in rels:\n",
    "            if score >= min_score:\n",
    "                evidence.append({\"target\": tgt, \"sentence\": sent, \"score\": score, \"pubYear\": row.get(\"pubYear\", \"\")})\n",
    "    return evidence\n",
    "\n",
    "# Example\n",
    "evidence = aggregate_relations(df_articles, min_score=0.5)\n",
    "# Summarize by target\n",
    "from collections import defaultdict\n",
    "agg = defaultdict(lambda: {\"count\":0, \"max_score\":0.0})\n",
    "for e in evidence:\n",
    "    agg[e[\"target\"]][\"count\"] += 1\n",
    "    agg[e[\"target\"]][\"max_score\"] = max(agg[e[\"target\"]][\"max_score\"], e[\"score\"])\n",
    "\n",
    "top_targets_by_rel = sorted([(t, v[\"count\"], v[\"max_score\"]) for t, v in agg.items()], key=lambda x: (-x[1], -x[2]))\n",
    "print(top_targets_by_rel[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "70150b03",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'streamlit'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstreamlit\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mst\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Load harmonized extraction results\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'streamlit'"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "\n",
    "# Load harmonized extraction results\n",
    "df_articles = pd.read_csv(\"output_with_normalized.csv\")\n",
    "\n",
    "# Collect all unique harmonized targets\n",
    "all_extracted_targets = set()\n",
    "for targets_str in df_articles['harmonized_text'].dropna():\n",
    "    if isinstance(targets_str, str):\n",
    "        targets = eval(targets_str)  # Only use eval on trusted data\n",
    "    else:\n",
    "        targets = targets_str\n",
    "    all_extracted_targets.update(targets)\n",
    "all_extracted_targets = sorted(list(all_extracted_targets))\n",
    "\n",
    "# Define the known drug targets (could come from a curated list, e.g. UniProt, HGNC, etc.)\n",
    "known_drug_targets = [\n",
    "    \"insulin receptor\", \"glucagon receptor\", \"GLP1R\", \"MTOR\", \"PPARγ\", \"TNF\", \"LEPR\",\n",
    "    # ...extend with your own list of drug targets\n",
    "]\n",
    "\n",
    "# --- APP UI ---\n",
    "st.title(\"Gene/Protein Target Selection App\")\n",
    "\n",
    "st.subheader(\"Extracted Targets From Data\")\n",
    "st.write(\"These are all harmonized targets found in your dataset (read-only):\")\n",
    "st.write(all_extracted_targets)\n",
    "\n",
    "st.subheader(\"Select Known Drug Creation Targets\")\n",
    "selected_targets = st.multiselect(\n",
    "    \"Choose drug targets for prioritization or further analysis:\",\n",
    "    known_drug_targets\n",
    ")\n",
    "\n",
    "st.write(\"### Your Selected Drug Targets\")\n",
    "if selected_targets:\n",
    "    st.write(selected_targets)\n",
    "else:\n",
    "    st.write(\"No drug targets selected.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8627795a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mygene\n",
      "  Downloading mygene-3.2.2-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting bioservices\n",
      "  Downloading bioservices-1.12.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting biothings-client>=0.2.6 (from mygene)\n",
      "  Downloading biothings_client-0.4.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: appdirs<2.0.0,>=1.4.4 in c:\\users\\aksha\\anaconda3\\envs\\psp\\lib\\site-packages (from bioservices) (1.4.4)\n",
      "Collecting beautifulsoup4<5.0.0,>=4.12.3 (from bioservices)\n",
      "  Downloading beautifulsoup4-4.14.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.8 in c:\\users\\aksha\\anaconda3\\envs\\psp\\lib\\site-packages (from bioservices) (8.3.0)\n",
      "Collecting colorlog<7.0.0,>=6.9.0 (from bioservices)\n",
      "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting easydev<0.14.0,>=0.13.3 (from bioservices)\n",
      "  Downloading easydev-0.13.3-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting grequests<0.8.0,>=0.7.0 (from bioservices)\n",
      "  Downloading grequests-0.7.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting lxml<6.0.0,>=5.3.0 (from bioservices)\n",
      "  Downloading lxml-5.4.0-cp311-cp311-win_amd64.whl.metadata (3.6 kB)\n",
      "Collecting matplotlib>=3.9 (from bioservices)\n",
      "  Downloading matplotlib-3.10.6-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pandas>2.2 in c:\\users\\aksha\\anaconda3\\envs\\psp\\lib\\site-packages (from bioservices) (2.3.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.3 in c:\\users\\aksha\\anaconda3\\envs\\psp\\lib\\site-packages (from bioservices) (2.32.5)\n",
      "Collecting requests-cache<2.0.0,>=1.2.1 (from bioservices)\n",
      "  Using cached requests_cache-1.2.1-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting rich-click<2.0.0,>=1.8.5 (from bioservices)\n",
      "  Downloading rich_click-1.9.1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting suds-community<2.0.0,>=1.2.0 (from bioservices)\n",
      "  Downloading suds_community-1.2.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.67.1 in c:\\users\\aksha\\anaconda3\\envs\\psp\\lib\\site-packages (from bioservices) (4.67.1)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.17.2 in c:\\users\\aksha\\anaconda3\\envs\\psp\\lib\\site-packages (from bioservices) (1.17.3)\n",
      "Collecting xmltodict<0.15.0,>=0.14.2 (from bioservices)\n",
      "  Downloading xmltodict-0.14.2-py2.py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4<5.0.0,>=4.12.3->bioservices)\n",
      "  Downloading soupsieve-2.8-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\aksha\\anaconda3\\envs\\psp\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.12.3->bioservices) (4.15.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\aksha\\anaconda3\\envs\\psp\\lib\\site-packages (from click<9.0.0,>=8.1.8->bioservices) (0.4.6)\n",
      "Collecting line-profiler<5.0.0,>=4.1.2 (from easydev<0.14.0,>=0.13.3->bioservices)\n",
      "  Downloading line_profiler-4.2.0-cp311-cp311-win_amd64.whl.metadata (35 kB)\n",
      "Collecting pexpect<5.0.0,>=4.9.0 (from easydev<0.14.0,>=0.13.3->bioservices)\n",
      "  Downloading pexpect-4.9.0-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: platformdirs<5.0.0,>=4.2.0 in c:\\users\\aksha\\anaconda3\\envs\\psp\\lib\\site-packages (from easydev<0.14.0,>=0.13.3->bioservices) (4.4.0)\n",
      "Collecting gevent (from grequests<0.8.0,>=0.7.0->bioservices)\n",
      "  Downloading gevent-25.9.1-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Collecting ptyprocess>=0.5 (from pexpect<5.0.0,>=4.9.0->easydev<0.14.0,>=0.13.3->bioservices)\n",
      "  Downloading ptyprocess-0.7.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\aksha\\anaconda3\\envs\\psp\\lib\\site-packages (from requests<3.0.0,>=2.32.3->bioservices) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aksha\\anaconda3\\envs\\psp\\lib\\site-packages (from requests<3.0.0,>=2.32.3->bioservices) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aksha\\anaconda3\\envs\\psp\\lib\\site-packages (from requests<3.0.0,>=2.32.3->bioservices) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aksha\\anaconda3\\envs\\psp\\lib\\site-packages (from requests<3.0.0,>=2.32.3->bioservices) (2025.8.3)\n",
      "Requirement already satisfied: attrs>=21.2 in c:\\users\\aksha\\anaconda3\\envs\\psp\\lib\\site-packages (from requests-cache<2.0.0,>=1.2.1->bioservices) (25.3.0)\n",
      "Collecting cattrs>=22.2 (from requests-cache<2.0.0,>=1.2.1->bioservices)\n",
      "  Downloading cattrs-25.2.0-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting url-normalize>=1.4 (from requests-cache<2.0.0,>=1.2.1->bioservices)\n",
      "  Using cached url_normalize-2.2.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting rich>=12 (from rich-click<2.0.0,>=1.8.5->bioservices)\n",
      "  Downloading rich-14.1.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting httpx>=0.22.0 (from biothings-client>=0.2.6->mygene)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting anyio (from httpx>=0.22.0->biothings-client>=0.2.6->mygene)\n",
      "  Using cached anyio-4.11.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting httpcore==1.* (from httpx>=0.22.0->biothings-client>=0.2.6->mygene)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx>=0.22.0->biothings-client>=0.2.6->mygene)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib>=3.9->bioservices)\n",
      "  Downloading contourpy-1.3.3-cp311-cp311-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib>=3.9->bioservices)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib>=3.9->bioservices)\n",
      "  Downloading fonttools-4.60.1-cp311-cp311-win_amd64.whl.metadata (114 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib>=3.9->bioservices)\n",
      "  Downloading kiwisolver-1.4.9-cp311-cp311-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\aksha\\anaconda3\\envs\\psp\\lib\\site-packages (from matplotlib>=3.9->bioservices) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aksha\\anaconda3\\envs\\psp\\lib\\site-packages (from matplotlib>=3.9->bioservices) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\aksha\\anaconda3\\envs\\psp\\lib\\site-packages (from matplotlib>=3.9->bioservices) (11.3.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib>=3.9->bioservices)\n",
      "  Downloading pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\aksha\\anaconda3\\envs\\psp\\lib\\site-packages (from matplotlib>=3.9->bioservices) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aksha\\anaconda3\\envs\\psp\\lib\\site-packages (from pandas>2.2->bioservices) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\aksha\\anaconda3\\envs\\psp\\lib\\site-packages (from pandas>2.2->bioservices) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aksha\\anaconda3\\envs\\psp\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.9->bioservices) (1.17.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=12->rich-click<2.0.0,>=1.8.5->bioservices)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\aksha\\anaconda3\\envs\\psp\\lib\\site-packages (from rich>=12->rich-click<2.0.0,>=1.8.5->bioservices) (2.19.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=12->rich-click<2.0.0,>=1.8.5->bioservices)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting sniffio>=1.1 (from anyio->httpx>=0.22.0->biothings-client>=0.2.6->mygene)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting greenlet>=3.2.2 (from gevent->grequests<0.8.0,>=0.7.0->bioservices)\n",
      "  Downloading greenlet-3.2.4-cp311-cp311-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: cffi>=1.17.1 in c:\\users\\aksha\\anaconda3\\envs\\psp\\lib\\site-packages (from gevent->grequests<0.8.0,>=0.7.0->bioservices) (2.0.0)\n",
      "Collecting zope.event (from gevent->grequests<0.8.0,>=0.7.0->bioservices)\n",
      "  Downloading zope_event-6.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting zope.interface (from gevent->grequests<0.8.0,>=0.7.0->bioservices)\n",
      "  Downloading zope_interface-8.0.1-cp311-cp311-win_amd64.whl.metadata (46 kB)\n",
      "Requirement already satisfied: pycparser in c:\\users\\aksha\\anaconda3\\envs\\psp\\lib\\site-packages (from cffi>=1.17.1->gevent->grequests<0.8.0,>=0.7.0->bioservices) (2.22)\n",
      "Requirement already satisfied: setuptools>=75.8.2 in c:\\users\\aksha\\anaconda3\\envs\\psp\\lib\\site-packages (from zope.event->gevent->grequests<0.8.0,>=0.7.0->bioservices) (80.9.0)\n",
      "Downloading mygene-3.2.2-py2.py3-none-any.whl (5.4 kB)\n",
      "Downloading bioservices-1.12.1-py3-none-any.whl (258 kB)\n",
      "Downloading beautifulsoup4-4.14.2-py3-none-any.whl (106 kB)\n",
      "Downloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Downloading easydev-0.13.3-py3-none-any.whl (57 kB)\n",
      "Downloading grequests-0.7.0-py2.py3-none-any.whl (5.7 kB)\n",
      "Downloading line_profiler-4.2.0-cp311-cp311-win_amd64.whl (128 kB)\n",
      "Downloading lxml-5.4.0-cp311-cp311-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.5/3.8 MB 4.2 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 1.3/3.8 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 2.4/3.8 MB 3.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 3.1/3.8 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.8/3.8 MB 3.7 MB/s  0:00:01\n",
      "Downloading pexpect-4.9.0-py2.py3-none-any.whl (63 kB)\n",
      "Using cached requests_cache-1.2.1-py3-none-any.whl (61 kB)\n",
      "Downloading rich_click-1.9.1-py3-none-any.whl (69 kB)\n",
      "Downloading suds_community-1.2.0-py3-none-any.whl (145 kB)\n",
      "Downloading xmltodict-0.14.2-py2.py3-none-any.whl (10.0 kB)\n",
      "Downloading biothings_client-0.4.1-py3-none-any.whl (46 kB)\n",
      "Downloading cattrs-25.2.0-py3-none-any.whl (70 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading matplotlib-3.10.6-cp311-cp311-win_amd64.whl (8.1 MB)\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.8/8.1 MB 4.2 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 1.6/8.1 MB 3.7 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.1/8.1 MB 3.6 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 2.9/8.1 MB 3.5 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 3.7/8.1 MB 3.5 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 4.2/8.1 MB 3.4 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 5.0/8.1 MB 3.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 5.5/8.1 MB 3.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 6.0/8.1 MB 3.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.6/8.1 MB 3.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.3/8.1 MB 3.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.9/8.1 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.1/8.1 MB 3.1 MB/s  0:00:02\n",
      "Downloading contourpy-1.3.3-cp311-cp311-win_amd64.whl (225 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.60.1-cp311-cp311-win_amd64.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.5/2.3 MB 4.2 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.3/2.3 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.3/2.3 MB 3.8 MB/s  0:00:00\n",
      "Downloading kiwisolver-1.4.9-cp311-cp311-win_amd64.whl (73 kB)\n",
      "Downloading ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n",
      "Downloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "Downloading rich-14.1.0-py3-none-any.whl (243 kB)\n",
      "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading soupsieve-2.8-py3-none-any.whl (36 kB)\n",
      "Using cached url_normalize-2.2.1-py3-none-any.whl (14 kB)\n",
      "Using cached anyio-4.11.0-py3-none-any.whl (109 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading gevent-25.9.1-cp311-cp311-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 0.8/1.7 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.6/1.7 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 3.1 MB/s  0:00:00\n",
      "Downloading greenlet-3.2.4-cp311-cp311-win_amd64.whl (299 kB)\n",
      "Downloading zope_event-6.0-py3-none-any.whl (6.4 kB)\n",
      "Downloading zope_interface-8.0.1-cp311-cp311-win_amd64.whl (212 kB)\n",
      "Installing collected packages: ptyprocess, zope.interface, zope.event, xmltodict, url-normalize, suds-community, soupsieve, sniffio, pyparsing, pexpect, mdurl, lxml, line-profiler, kiwisolver, h11, greenlet, fonttools, cycler, contourpy, colorlog, cattrs, requests-cache, matplotlib, markdown-it-py, httpcore, gevent, easydev, beautifulsoup4, anyio, rich, httpx, grequests, rich-click, biothings-client, mygene, bioservices\n",
      "\n",
      "   - --------------------------------------  1/36 [zope.interface]\n",
      "   ---- -----------------------------------  4/36 [url-normalize]\n",
      "   ----- ----------------------------------  5/36 [suds-community]\n",
      "   -------- -------------------------------  8/36 [pyparsing]\n",
      "   ---------- -----------------------------  9/36 [pexpect]\n",
      "   ------------ --------------------------- 11/36 [lxml]\n",
      "   ------------- -------------------------- 12/36 [line-profiler]\n",
      "   ---------------- ----------------------- 15/36 [greenlet]\n",
      "   ----------------- ---------------------- 16/36 [fonttools]\n",
      "   ----------------- ---------------------- 16/36 [fonttools]\n",
      "   ----------------- ---------------------- 16/36 [fonttools]\n",
      "   ----------------- ---------------------- 16/36 [fonttools]\n",
      "   ----------------- ---------------------- 16/36 [fonttools]\n",
      "   ----------------- ---------------------- 16/36 [fonttools]\n",
      "   ----------------- ---------------------- 16/36 [fonttools]\n",
      "   ----------------- ---------------------- 16/36 [fonttools]\n",
      "   ----------------- ---------------------- 16/36 [fonttools]\n",
      "   ----------------- ---------------------- 16/36 [fonttools]\n",
      "   -------------------- ------------------- 18/36 [contourpy]\n",
      "   ---------------------- ----------------- 20/36 [cattrs]\n",
      "   ----------------------- ---------------- 21/36 [requests-cache]\n",
      "   ------------------------ --------------- 22/36 [matplotlib]\n",
      "   ------------------------ --------------- 22/36 [matplotlib]\n",
      "   ------------------------ --------------- 22/36 [matplotlib]\n",
      "   ------------------------ --------------- 22/36 [matplotlib]\n",
      "   ------------------------ --------------- 22/36 [matplotlib]\n",
      "   ------------------------ --------------- 22/36 [matplotlib]\n",
      "   ------------------------ --------------- 22/36 [matplotlib]\n",
      "   ------------------------ --------------- 22/36 [matplotlib]\n",
      "   ------------------------ --------------- 22/36 [matplotlib]\n",
      "   ------------------------ --------------- 22/36 [matplotlib]\n",
      "   ------------------------ --------------- 22/36 [matplotlib]\n",
      "   ------------------------ --------------- 22/36 [matplotlib]\n",
      "   ------------------------ --------------- 22/36 [matplotlib]\n",
      "   ------------------------ --------------- 22/36 [matplotlib]\n",
      "   ------------------------ --------------- 22/36 [matplotlib]\n",
      "   ------------------------ --------------- 22/36 [matplotlib]\n",
      "   ------------------------- -------------- 23/36 [markdown-it-py]\n",
      "   -------------------------- ------------- 24/36 [httpcore]\n",
      "   --------------------------- ------------ 25/36 [gevent]\n",
      "   --------------------------- ------------ 25/36 [gevent]\n",
      "   --------------------------- ------------ 25/36 [gevent]\n",
      "   --------------------------- ------------ 25/36 [gevent]\n",
      "   --------------------------- ------------ 25/36 [gevent]\n",
      "   --------------------------- ------------ 25/36 [gevent]\n",
      "   --------------------------- ------------ 25/36 [gevent]\n",
      "   ------------------------------ --------- 27/36 [beautifulsoup4]\n",
      "   ------------------------------- -------- 28/36 [anyio]\n",
      "   -------------------------------- ------- 29/36 [rich]\n",
      "   -------------------------------- ------- 29/36 [rich]\n",
      "   --------------------------------- ------ 30/36 [httpx]\n",
      "   ----------------------------------- ---- 32/36 [rich-click]\n",
      "   -------------------------------------- - 35/36 [bioservices]\n",
      "   -------------------------------------- - 35/36 [bioservices]\n",
      "   ---------------------------------------- 36/36 [bioservices]\n",
      "\n",
      "Successfully installed anyio-4.11.0 beautifulsoup4-4.14.2 bioservices-1.12.1 biothings-client-0.4.1 cattrs-25.2.0 colorlog-6.9.0 contourpy-1.3.3 cycler-0.12.1 easydev-0.13.3 fonttools-4.60.1 gevent-25.9.1 greenlet-3.2.4 grequests-0.7.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 kiwisolver-1.4.9 line-profiler-4.2.0 lxml-5.4.0 markdown-it-py-4.0.0 matplotlib-3.10.6 mdurl-0.1.2 mygene-3.2.2 pexpect-4.9.0 ptyprocess-0.7.0 pyparsing-3.2.5 requests-cache-1.2.1 rich-14.1.0 rich-click-1.9.1 sniffio-1.3.1 soupsieve-2.8 suds-community-1.2.0 url-normalize-2.2.1 xmltodict-0.14.2 zope.event-6.0 zope.interface-8.0.1\n",
      "Creating directory C:\\Users\\aksha\\AppData\\Local\\bioservices\\bioservices \n",
      "Creating directory C:\\Users\\aksha\\AppData\\Local\\bioservices\\bioservices\\Cache \n",
      "Welcome to Bioservices\n",
      "======================\n",
      "It looks like you do not have a configuration file.\n",
      "We are creating one with default values in C:\\Users\\aksha\\AppData\\Local\\bioservices\\bioservices\\bioservices.cfg .\n",
      "Done\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'entities' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 68\u001b[39m\n\u001b[32m     65\u001b[39m results = []\n\u001b[32m     66\u001b[39m official_names = {}\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m term \u001b[38;5;129;01min\u001b[39;00m \u001b[43mentities\u001b[49m:\n\u001b[32m     69\u001b[39m     norm = gilda.ground(term)\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m norm:\n",
      "\u001b[31mNameError\u001b[39m: name 'entities' is not defined"
     ]
    }
   ],
   "source": [
    "!pip install mygene bioservices\n",
    "import pandas as pd\n",
    "import gilda\n",
    "import mygene\n",
    "from bioservices import UniProt\n",
    "\n",
    "# Set up lookup services\n",
    "mg = mygene.MyGeneInfo()\n",
    "u = UniProt()\n",
    "\n",
    "def lookup_gene_symbol(database, id_):\n",
    "    \"\"\"\n",
    "    Given HGNC database and ID, return canonical gene symbol or name.\n",
    "    \"\"\"\n",
    "    if database == \"HGNC\" and pd.notnull(id_):\n",
    "        try:\n",
    "            result = mg.getgene(id_, fields=\"symbol,name\")\n",
    "            if result is not None:\n",
    "                # Prefer symbol, fallback to name\n",
    "                return result.get(\"symbol\") or result.get(\"name\")\n",
    "        except Exception as e:\n",
    "            print(f\"Gene lookup error for {id_}: {e}\")\n",
    "    return None\n",
    "\n",
    "def lookup_protein_name(database, id_):\n",
    "    \"\"\"\n",
    "    Given UniProt db (UP) and ID, return recommended protein name.\n",
    "    \"\"\"\n",
    "    if database == \"UP\" and pd.notnull(id_):\n",
    "        try:\n",
    "            # The response is a tab-separated table: ID\\tProtein names\n",
    "            result = u.search(id_, frmt=\"tab\", columns=\"id,protein names\", limit=1)\n",
    "            lines = result.strip().split(\"\\n\")\n",
    "            if len(lines) > 1:\n",
    "                # Second line is data row\n",
    "                return lines[1].split(\"\\t\")[1]\n",
    "        except Exception as e:\n",
    "            print(f\"UniProt lookup error for {id_}: {e}\")\n",
    "    return None\n",
    "\n",
    "def harmonize_name(row):\n",
    "    \"\"\"\n",
    "    Returns the harmonized entity name, using external services if needed.\n",
    "    \"\"\"\n",
    "    db, id_ = row[\"database\"], row[\"id\"]\n",
    "    # Gene harmonization\n",
    "    if db == \"HGNC\":\n",
    "        canonical = lookup_gene_symbol(db, id_)\n",
    "        if canonical:\n",
    "            return canonical\n",
    "    # UniProt protein harmonization\n",
    "    if db == \"UP\":\n",
    "        canonical = lookup_protein_name(db, id_)\n",
    "        if canonical:\n",
    "            return canonical\n",
    "    # FPLX, CHEBI, MESH, EFO fallbacks: use normalized_name if present\n",
    "    if db in [\"FPLX\", \"CHEBI\", \"MESH\", \"EFO\"]:\n",
    "        return row[\"normalized_name\"] or row[\"official_name\"] or row[\"original\"]\n",
    "    # Fallback: use whatever is available\n",
    "    return row[\"official_name\"] or row[\"normalized_name\"] or row[\"original\"]\n",
    "\n",
    "# Your entity list (replace with your actual source)\n",
    "# entities = [term for term, freq in top_targets]\n",
    "\n",
    "results = []\n",
    "official_names = {}\n",
    "\n",
    "for term in entities:\n",
    "    norm = gilda.ground(term)\n",
    "    if norm:\n",
    "        best = norm[0]\n",
    "        entry_obj = getattr(best, 'entry', None)\n",
    "        official_name = None\n",
    "        if entry_obj is not None:\n",
    "            official_name = getattr(entry_obj, \"name\", None) or getattr(entry_obj, \"label\", None)\n",
    "        if not official_name and hasattr(entry_obj, \"names\"):\n",
    "            official_name = entry_obj.names[0] if entry_obj.names else None\n",
    "        if not official_name:\n",
    "            official_name = str(entry_obj)  # fallback\n",
    "\n",
    "        print(f\"DEBUG: {term} => official_name: {official_name}; repr(entry): {repr(entry_obj)}\")\n",
    "        groundings = best.get_groundings()\n",
    "        if groundings:\n",
    "            db, id_ = next(iter(groundings))\n",
    "        else:\n",
    "            db, id_ = None, None\n",
    "\n",
    "        # Try to extract official name from match entry\n",
    "        official_name = getattr(best.entry, 'name', None) if hasattr(best, 'entry') else None\n",
    "\n",
    "        results.append({\n",
    "            'original': term,\n",
    "            'normalized_name': best.term.name if hasattr(best.term, \"name\") else str(best.term),\n",
    "            'database': db,\n",
    "            'id': id_,\n",
    "            'score': best.score,\n",
    "            'official_name': official_name\n",
    "        })\n",
    "\n",
    "        key = (db, id_)\n",
    "        if official_name and key not in official_names:\n",
    "            official_names[key] = official_name\n",
    "\n",
    "    else:\n",
    "        results.append({\n",
    "            'original': term,\n",
    "            'normalized_name': None,\n",
    "            'database': None,\n",
    "            'id': None,\n",
    "            'score': None,\n",
    "            'official_name': None\n",
    "        })\n",
    "\n",
    "df_norm = pd.DataFrame(results)\n",
    "\n",
    "# Harmonized names using external lookup or fallbacks\n",
    "df_norm['harmonized_name'] = df_norm.apply(harmonize_name, axis=1)\n",
    "\n",
    "print(df_norm)\n",
    "df_norm.to_csv(\"normalized_targets_harmonized.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "366a06c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'GENETIC', 'score': np.float32(0.9999895), 'word': 'pulmonary ceramide transfer protein', 'start': 719, 'end': 754}, {'entity_group': 'GENETIC', 'score': np.float32(0.99985635), 'word': 'cert', 'start': 756, 'end': 760}, {'entity_group': 'GENETIC', 'score': np.float32(0.99662757), 'word': 'cer', 'start': 832, 'end': 835}, {'entity_group': 'GENETIC', 'score': np.float32(0.9998863), 'word': 'cert', 'start': 907, 'end': 911}, {'entity_group': 'GENETIC', 'score': np.float32(0.999957), 'word': 'cert', 'start': 1005, 'end': 1009}, {'entity_group': 'GENETIC', 'score': np.float32(0.98633677), 'word': 'cer', 'start': 1040, 'end': 1043}, {'entity_group': 'GENETIC', 'score': np.float32(0.99900615), 'word': 'cer', 'start': 1074, 'end': 1077}, {'entity_group': 'GENETIC', 'score': np.float32(0.99995786), 'word': 'cert', 'start': 1243, 'end': 1247}, {'entity_group': 'GENETIC', 'score': np.float32(0.99996424), 'word': 'cert', 'start': 1379, 'end': 1383}, {'entity_group': 'GENETIC', 'score': np.float32(0.9999056), 'word': 'cert', 'start': 1478, 'end': 1482}, {'entity_group': 'GENETIC', 'score': np.float32(0.99872285), 'word': 'cer', 'start': 1491, 'end': 1494}, {'entity_group': 'GENETIC', 'score': np.float32(0.939674), 'word': 'cer', 'start': 1516, 'end': 1519}, {'entity_group': 'GENETIC', 'score': np.float32(0.9997674), 'word': 'cert', 'start': 1635, 'end': 1639}]\n",
      "Unique entity types found: {'GENETIC'}\n"
     ]
    }
   ],
   "source": [
    "# Test on a sample abstract\n",
    "sample_text = df_articles[\"abstract\"].dropna().iloc[0]\n",
    "ner_results = nlp_ner(sample_text)\n",
    "print(ner_results)\n",
    "\n",
    "# Print all unique entity groups found\n",
    "unique_entities = set([entity['entity_group'] for entity in ner_results])\n",
    "print(\"Unique entity types found:\", unique_entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2954bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'O', 1: 'B-Activity', 2: 'B-Administration', 3: 'B-Age', 4: 'B-Area', 5: 'B-Biological_attribute', 6: 'B-Biological_structure', 7: 'B-Clinical_event', 8: 'B-Color', 9: 'B-Coreference', 10: 'B-Date', 11: 'B-Detailed_description', 12: 'B-Diagnostic_procedure', 13: 'B-Disease_disorder', 14: 'B-Distance', 15: 'B-Dosage', 16: 'B-Duration', 17: 'B-Family_history', 18: 'B-Frequency', 19: 'B-Height', 20: 'B-History', 21: 'B-Lab_value', 22: 'B-Mass', 23: 'B-Medication', 24: 'B-Non[biological](Detailed_description', 25: 'B-Nonbiological_location', 26: 'B-Occupation', 27: 'B-Other_entity', 28: 'B-Other_event', 29: 'B-Outcome', 30: 'B-Personal_[back](Biological_structure', 31: 'B-Personal_background', 32: 'B-Qualitative_concept', 33: 'B-Quantitative_concept', 34: 'B-Severity', 35: 'B-Sex', 36: 'B-Shape', 37: 'B-Sign_symptom', 38: 'B-Subject', 39: 'B-Texture', 40: 'B-Therapeutic_procedure', 41: 'B-Time', 42: 'B-Volume', 43: 'B-Weight', 44: 'I-Activity', 45: 'I-Administration', 46: 'I-Age', 47: 'I-Area', 48: 'I-Biological_attribute', 49: 'I-Biological_structure', 50: 'I-Clinical_event', 51: 'I-Color', 52: 'I-Coreference', 53: 'I-Date', 54: 'I-Detailed_description', 55: 'I-Diagnostic_procedure', 56: 'I-Disease_disorder', 57: 'I-Distance', 58: 'I-Dosage', 59: 'I-Duration', 60: 'I-Family_history', 61: 'I-Frequency', 62: 'I-Height', 63: 'I-History', 64: 'I-Lab_value', 65: 'I-Mass', 66: 'I-Medication', 67: 'I-Nonbiological_location', 68: 'I-Occupation', 69: 'I-Other_entity', 70: 'I-Other_event', 71: 'I-Outcome', 72: 'I-Personal_background', 73: 'I-Qualitative_concept', 74: 'I-Quantitative_concept', 75: 'I-Severity', 76: 'I-Shape', 77: 'I-Sign_symptom', 78: 'I-Subject', 79: 'I-Texture', 80: 'I-Therapeutic_procedure', 81: 'I-Time', 82: 'I-Volume', 83: 'I-Weight'}\n"
     ]
    }
   ],
   "source": [
    "print(model.config.id2label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61845bf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "psp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
